{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anonymous/anaconda3/envs/nursingllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detgpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaForCausalLM, LlamaTokenizer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Blip2Base\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDetGPT\u001b[39;00m(Blip2Base):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vit_model, q_former_model, img_size, llama_model, prompt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prompt_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_txt_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, end_sym\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, low_resource\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m~/桌面/NursingLLM/models/blip2.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdetgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdist_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist_utils\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdist_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_cached_file\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetgpt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_url\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detgpt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from models.blip2 import Blip2Base\n",
    "\n",
    "class DetGPT(Blip2Base):\n",
    "    def __init__(self, vit_model, q_former_model, img_size, llama_model, prompt_path=None, prompt_template=\"\", max_txt_len=32, end_sym='\\n', low_resource=False, device_8bit=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(llama_model, use_fast=False)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.visual_encoder, self.ln_vision = self.init_vision_encoder(vit_model, img_size)\n",
    "        self.Qformer, self.query_tokens = self.init_Qformer(num_query_token=32, vision_width=self.visual_encoder.num_features)\n",
    "        self.load_from_pretrained(url_or_filename=q_former_model)\n",
    "\n",
    "        self.llama_model = LlamaForCausalLM.from_pretrained(llama_model, torch_dtype=torch.float16, load_in_8bit=low_resource, device_map={'': device_8bit} if low_resource else None)\n",
    "        self.llama_proj = nn.Linear(self.Qformer.config.hidden_size, self.llama_model.config.hidden_size)\n",
    "\n",
    "        self.max_txt_len = max_txt_len\n",
    "        self.end_sym = end_sym\n",
    "\n",
    "        if prompt_path:\n",
    "            with open(prompt_path, 'r') as f:\n",
    "                raw_prompts = f.read().splitlines()\n",
    "            self.prompt_list = [prompt_template.format(p) for p in raw_prompts if \"<ImageHere>\" in p]\n",
    "\n",
    "    def encode_img(self, image):\n",
    "        device = image.device\n",
    "        image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)\n",
    "        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "        query_output = self.Qformer.bert(query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True)\n",
    "        inputs_llama = self.llama_proj(query_output.last_hidden_state)\n",
    "        atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)\n",
    "        return inputs_llama, atts_llama\n",
    "\n",
    "    def prompt_wrap(self, img_embeds, atts_img, prompt):\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        p_before, p_after = prompt.split('<ImageHere>')\n",
    "        p_before_tokens = self.tokenizer(p_before, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "        p_after_tokens = self.tokenizer(p_after, return_tensors=\"pt\", add_special_tokens=False).to(img_embeds.device)\n",
    "        p_before_embeds = self.llama_model.model.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "        p_after_embeds = self.llama_model.model.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "        wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)\n",
    "        wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])\n",
    "        return wrapped_img_embeds, wrapped_atts_img\n",
    "\n",
    "    def forward(self, samples):\n",
    "        image = samples[\"image\"]\n",
    "        img_embeds, atts_img = self.encode_img(image)\n",
    "\n",
    "        if \"task\" in samples:\n",
    "            task = samples['task']\n",
    "            task_prompt = ['<ImageHere> ' + t for t in task]\n",
    "            img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, random.choice(task_prompt))\n",
    "            text_input = samples['text_input']\n",
    "\n",
    "        text = [t + self.end_sym for t in text_input]\n",
    "        to_regress_tokens = self.tokenizer(text, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=self.max_txt_len, add_special_tokens=False).to(image.device)\n",
    "        targets = to_regress_tokens.input_ids.masked_fill(to_regress_tokens.input_ids == self.tokenizer.pad_token_id, -100)\n",
    "\n",
    "        empty_targets = torch.ones([atts_img.shape[0], atts_img.shape[1]+1], dtype=torch.long).to(image.device).fill_(-100)\n",
    "        targets = torch.cat([empty_targets, targets], dim=1)\n",
    "\n",
    "        batch_size = img_embeds.shape[0]\n",
    "        bos = torch.ones([batch_size, 1], dtype=to_regress_tokens.input_ids.dtype, device=to_regress_tokens.input_ids.device) * self.tokenizer.bos_token_id\n",
    "        bos_embeds = self.llama_model.model.embed_tokens(bos)\n",
    "        atts_bos = atts_img[:, :1]\n",
    "\n",
    "        to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)\n",
    "        inputs_embeds = torch.cat([bos_embeds, img_embeds, to_regress_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_bos, atts_img, to_regress_tokens.attention_mask], dim=1)\n",
    "\n",
    "        outputs = self.llama_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, return_dict=True, labels=targets)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        return cls(\n",
    "            vit_model=cfg.get(\"vit_model\", \"eva_clip_g\"),\n",
    "            q_former_model=cfg.get(\"q_former_model\", \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth\"),\n",
    "            img_size=cfg.get(\"image_size\", 224),\n",
    "            llama_model=cfg.get(\"llama_model\", \"./vicuna-7b-v1.5\"),\n",
    "            prompt_path=cfg.get(\"prompt_path\", \"\"),\n",
    "            prompt_template=cfg.get(\"prompt_template\", \"\"),\n",
    "            max_txt_len=cfg.get(\"max_txt_len\", 32),\n",
    "            end_sym=cfg.get(\"end_sym\", '\\n'),\n",
    "            low_resource=cfg.get(\"low_resource\", False),\n",
    "            device_8bit=cfg.get(\"device_8bit\", 0)\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nursingllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
