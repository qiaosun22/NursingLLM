{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anonymous/anaconda3/envs/nursingllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.34it/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 12879.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, TaskType\n",
    "import torch\n",
    "# import deepspeed\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct', use_fast=False, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\", use_fast=False, trust_remote_code=True)\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct', device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "\n",
    "# 加载自定义格式的JSON文件\n",
    "with open('custom_conversations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 自定义数据处理函数\n",
    "def process_conversations(data):\n",
    "    conversations = []\n",
    "    for conv in data:\n",
    "        for turn in conv['chat']:\n",
    "            conversations.append({\n",
    "                \"conversation_id\": conv[\"id\"],\n",
    "                \"turn_id\": turn[\"id\"],\n",
    "                \"user\": turn[\"user_msg\"],\n",
    "                \"assistant\": turn[\"bot_response\"]\n",
    "            })\n",
    "    return conversations\n",
    "\n",
    "# 应用数据处理函数\n",
    "processed_data = process_conversations(data)\n",
    "\n",
    "# 创建Dataset对象\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "\n",
    "# # 选择预训练模型的分词器\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\")\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['user'], examples['assistant'], padding=\"max_length\", truncation=True, max_length=100)\n",
    "    # Create labels\n",
    "    labels = model_inputs['input_ids'].copy()\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# 对数据集进行分词和编码\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 将数据集拆分为训练集和验证集\n",
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 使用DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/llama3\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=False,\n",
    "    gradient_checkpointing=False\n",
    ")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你好\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nursingllm/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "model.chat(tokenizer, \"你好\", history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/anonymous/anaconda3/envs/nursingllm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m      5\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nursingllm/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nursingllm/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/nursingllm/lib/python3.10/site-packages/transformers/trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nursingllm/lib/python3.10/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者：亚东\n",
    "# 链接：https://zhuanlan.zhihu.com/p/700844670\n",
    "# 来源：知乎\n",
    "# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "import torch\n",
    "# from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, TaskType\n",
    "import torch\n",
    "# import deepspeed\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# model_dir = snapshot_download('LLM-Research/Meta-Llama-3-8B-Instruct', cache_dir='/root/autodl-tmp', revision='master')\n",
    "\n",
    "# def process_func(example):\n",
    "#     MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "#     input_ids, attention_mask, labels = [], [], []\n",
    "#     instruction = tokenizer(f\"<|start_header_id|>user<|end_header_id|>\\n\\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "#     response = tokenizer(f\"{example['output']}<|eot_id|>\", add_special_tokens=False)\n",
    "#     input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "#     attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "#     labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "#     if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "#         input_ids = input_ids[:MAX_LENGTH]\n",
    "#         attention_mask = attention_mask[:MAX_LENGTH]\n",
    "#         labels = labels[:MAX_LENGTH]\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "    \n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct', use_fast=False, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\", use_fast=False, trust_remote_code=True)\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct', device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "\n",
    "# 加载自定义格式的JSON文件\n",
    "with open('custom_conversations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 自定义数据处理函数\n",
    "def process_conversations(data):\n",
    "    conversations = []\n",
    "    for conv in data:\n",
    "        for turn in conv['chat']:\n",
    "            conversations.append({\n",
    "                \"conversation_id\": conv[\"id\"],\n",
    "                \"turn_id\": turn[\"id\"],\n",
    "                \"user\": turn[\"user_msg\"],\n",
    "                \"assistant\": turn[\"bot_response\"]\n",
    "            })\n",
    "    return conversations\n",
    "\n",
    "# 应用数据处理函数\n",
    "processed_data = process_conversations(data)\n",
    "\n",
    "# 创建Dataset对象\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "\n",
    "# # 选择预训练模型的分词器\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\")\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['user'], examples['assistant'], padding=\"max_length\", truncation=True, max_length=100)\n",
    "    # Create labels\n",
    "    labels = model_inputs['input_ids'].copy()\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# 对数据集进行分词和编码\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 将数据集拆分为训练集和验证集\n",
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "# 使用DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/llama3\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=False,\n",
    "    gradient_checkpointing=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# lora_path='./llama3_lora'\n",
    "# trainer.model.save_pretrained(lora_path)\n",
    "# tokenizer.save_pretrained(lora_path)\n",
    "\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# # mode_path = '/root/autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct'\n",
    "# lora_path = './llama3_lora' # lora权重路径\n",
    "\n",
    "# # 加载tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(mode_path)\n",
    "\n",
    "# # 加载模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "\n",
    "# # 加载lora权重\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path, config=config)\n",
    "\n",
    "# prompt = \"你是谁？\"\n",
    "# messages = [\n",
    "#     # {\"role\": \"system\", \"content\": \"现在你要扮演皇帝身边的女人--甄嬛\"},\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     model_inputs.input_ids,\n",
    "#     max_new_tokens=512,\n",
    "#     eos_token_id=tokenizer.encode('<|eot_id|>')[0]\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# print(response)\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "# import torch\n",
    "# import deepspeed\n",
    "\n",
    "# # Set a different port for distributed training\n",
    "# os.environ['MASTER_PORT'] = '29501'  # Change this to an available port number\n",
    "\n",
    "# # 加载自定义格式的JSON文件\n",
    "# with open('custom_conversations.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # 自定义数据处理函数\n",
    "# def process_conversations(data):\n",
    "#     conversations = []\n",
    "#     for conv in data:\n",
    "#         for turn in conv['chat']:\n",
    "#             conversations.append({\n",
    "#                 \"conversation_id\": conv[\"id\"],\n",
    "#                 \"turn_id\": turn[\"id\"],\n",
    "#                 \"user\": turn[\"user_msg\"],\n",
    "#                 \"assistant\": turn[\"bot_response\"]\n",
    "#             })\n",
    "#     return conversations\n",
    "\n",
    "# # 应用数据处理函数\n",
    "# processed_data = process_conversations(data)\n",
    "\n",
    "# # 创建Dataset对象\n",
    "# dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "\n",
    "# # 选择预训练模型的分词器\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\")\n",
    "\n",
    "# # 定义分词函数\n",
    "# def tokenize_function(examples):\n",
    "#     model_inputs = tokenizer(examples['user'], examples['assistant'], padding=\"max_length\", truncation=True, max_length=100)\n",
    "#     # Create labels\n",
    "#     labels = model_inputs['input_ids'].copy()\n",
    "#     model_inputs['labels'] = labels\n",
    "#     return model_inputs\n",
    "\n",
    "# # 对数据集进行分词和编码\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # 将数据集拆分为训练集和验证集\n",
    "# split_dataset = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "# train_dataset = split_dataset['train']\n",
    "# eval_dataset = split_dataset['test']\n",
    "\n",
    "# # 使用DataCollatorForLanguageModeling\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # 选择预训练模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\")\n",
    "\n",
    "# # 设置训练参数\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=3e-7,\n",
    "#     save_total_limit=1,\n",
    "#     fp16=True,  # 启用半精度训练\n",
    "#     dataloader_num_workers=4,\n",
    "#     deepspeed=\"ds_config.json\",  # 添加DeepSpeed配置文件进行分布式训练\n",
    "# )\n",
    "\n",
    "# # 定义Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# # 初始化 DeepSpeed\n",
    "# deepspeed.init_distributed()\n",
    "\n",
    "# # 开始训练\n",
    "# trainer.train()\n",
    "\n",
    "# # 保存模型\n",
    "# model.save_pretrained(\"./fine_tuned_llama3\")\n",
    "# tokenizer.save_pretrained(\"./fine_tuned_llama3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # import json\n",
    "# # from datasets import Dataset, DatasetDict\n",
    "# # import pandas as pd\n",
    "# # from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "# # import torch\n",
    "# # import os\n",
    "\n",
    "\n",
    "# # # Set a different port for distributed training\n",
    "# # os.environ['MASTER_PORT'] = '29501'  # Change this to an available port number\n",
    "\n",
    "\n",
    "# # # 加载自定义格式的JSON文件\n",
    "# # with open('custom_conversations.json', 'r') as f:\n",
    "# #     data = json.load(f)\n",
    "\n",
    "# # # 自定义数据处理函数\n",
    "# # def process_conversations(data):\n",
    "# #     conversations = []\n",
    "# #     for conv in data:\n",
    "# #         for turn in conv['chat']:\n",
    "# #             conversations.append({\n",
    "# #                 \"conversation_id\": conv[\"id\"],\n",
    "# #                 \"turn_id\": turn[\"id\"],\n",
    "# #                 \"user\": turn[\"user_msg\"],\n",
    "# #                 \"assistant\": turn[\"bot_response\"]\n",
    "# #             })\n",
    "# #     return conversations\n",
    "\n",
    "# # # 应用数据处理函数\n",
    "# # processed_data = process_conversations(data)\n",
    "\n",
    "# # # 创建Dataset对象\n",
    "# # dataset = Dataset.from_pandas(pd.DataFrame(processed_data))\n",
    "\n",
    "# # # 选择预训练模型的分词器\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\")\n",
    "\n",
    "# # # 定义分词函数\n",
    "# # def tokenize_function(examples):\n",
    "# #     model_inputs = tokenizer(examples['user'], examples['assistant'], padding=\"max_length\", truncation=True, max_length=100)\n",
    "# #     # Create labels\n",
    "# #     labels = model_inputs['input_ids'].copy()\n",
    "# #     model_inputs['labels'] = labels\n",
    "# #     return model_inputs\n",
    "\n",
    "# # # 对数据集进行分词和编码\n",
    "# # tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # # 将数据集拆分为训练集和验证集\n",
    "# # split_dataset = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "# # train_dataset = split_dataset['train']\n",
    "# # eval_dataset = split_dataset['test']\n",
    "\n",
    "# # # 使用DataCollatorForLanguageModeling\n",
    "# # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # # 将模型移动到GPU（如果可用）\n",
    "# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # # 选择预训练模型\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\"shenzhi-wang/Llama3-8B-Chinese-Chat\").to(device).half()\n",
    "\n",
    "# # # 设置训练参数\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir=\"./results\",\n",
    "# #     evaluation_strategy=\"epoch\",\n",
    "# #     learning_rate=2e-5,\n",
    "# #     per_device_train_batch_size=1,\n",
    "# #     per_device_eval_batch_size=1,\n",
    "# #     num_train_epochs=3,\n",
    "# #     weight_decay=3e-7,\n",
    "# #     save_total_limit=1,\n",
    "# #     fp16=True,  # 启用半精度训练\n",
    "# #     dataloader_num_workers=4,\n",
    "# #     deepspeed=\"ds_config.json\",  # 添加DeepSpeed配置文件进行分布式训练\n",
    "# # )\n",
    "\n",
    "# # # 定义Trainer\n",
    "# # trainer = Trainer(\n",
    "# #     model=model,\n",
    "# #     args=training_args,\n",
    "# #     train_dataset=train_dataset,\n",
    "# #     eval_dataset=eval_dataset,\n",
    "# #     data_collator=data_collator,\n",
    "# #     tokenizer=tokenizer,\n",
    "# # )\n",
    "\n",
    "# # # 开始训练\n",
    "# # trainer.train()\n",
    "\n",
    "# # # 保存模型\n",
    "# # model.save_pretrained(\"./fine_tuned_llama3\")\n",
    "# # tokenizer.save_pretrained(\"./fine_tuned_llama3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nursingllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
